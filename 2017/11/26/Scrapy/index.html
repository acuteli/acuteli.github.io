<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <title>Hexo</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="###爬虫1234567url模块综合url和lib的一个包 第一部分是协议：http,https,ftp,file,ed2k…第二部分是存放资源的服务器的域名系统或IP地址（有时候要包含端口号，各种传输协议都有默认的端口，如http的默认端口是80）第三部分是资源的具体地址，如目录或者文件名等 ##python中操作">
<meta property="og:type" content="article">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="http://yoursite.com/2017/11/26/Scrapy/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="###爬虫1234567url模块综合url和lib的一个包 第一部分是协议：http,https,ftp,file,ed2k…第二部分是存放资源的服务器的域名系统或IP地址（有时候要包含端口号，各种传输协议都有默认的端口，如http的默认端口是80）第三部分是资源的具体地址，如目录或者文件名等 ##python中操作文件步骤12345671. 打开文件，得到文件句柄并赋值给一个变量  f.ope">
<meta property="og:updated_time" content="2017-11-24T05:44:16.201Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Hexo">
<meta name="twitter:description" content="###爬虫1234567url模块综合url和lib的一个包 第一部分是协议：http,https,ftp,file,ed2k…第二部分是存放资源的服务器的域名系统或IP地址（有时候要包含端口号，各种传输协议都有默认的端口，如http的默认端口是80）第三部分是资源的具体地址，如目录或者文件名等 ##python中操作文件步骤12345671. 打开文件，得到文件句柄并赋值给一个变量  f.ope">
  
    <link rel="alternate" href="/atom.xml" title="Hexo" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
  

</head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hexo</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Scrapy" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2017/11/26/Scrapy/" class="article-date">
  <time datetime="2017-11-26T08:12:40.419Z" itemprop="datePublished">2017-11-26</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>###爬虫<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">url模块综合url和lib的一个包</span><br><span class="line"> </span><br><span class="line">第一部分是协议：http,https,ftp,file,ed2k…</span><br><span class="line"></span><br><span class="line">第二部分是存放资源的服务器的域名系统或IP地址（有时候要包含端口号，各种传输协议都有默认的端口，如http的默认端口是80）</span><br><span class="line"></span><br><span class="line">第三部分是资源的具体地址，如目录或者文件名等</span><br></pre></td></tr></table></figure></p>
<p>##python中操作文件步骤<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">1. 打开文件，得到文件句柄并赋值给一个变量 </span><br><span class="line"> f.open(&apos;a.txt&apos;,&apos;w&apos;,encoding=&apos;utf-8&apos;)</span><br><span class="line">2. 通过句柄对文件进行操作</span><br><span class="line">  data = f.read()</span><br><span class="line">3. 关闭文件</span><br><span class="line"> f.close()</span><br></pre></td></tr></table></figure></p>
<p>###计算机操作系统分为:硬件,操作系统,应用程序</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">工作过程:</span><br><span class="line">由应用程序向操作系统发起系统调用open();</span><br><span class="line">操作系统打开文件,返回文件句柄给应用程序;</span><br><span class="line">应用程序赋值给变量</span><br></pre></td></tr></table></figure>
<blockquote>
<p>selenium自动下拉,模仿按键精灵</p>
<p>MVC 模型视图控制器</p>
</blockquote>
<p>###HTTP<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HTTP协议是一个无状态的协议，同一个客户端的这次请求和上次请求是没有对应关系。</span><br><span class="line"></span><br><span class="line">MIME:设定某种扩展名的文件用一种应用程序来打开的方式类型，当该扩展名文件被访问的时候，浏览器会自动使用指定应用程序来打开</span><br></pre></td></tr></table></figure></p>
<p>###HTTP工作流程:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1.客户机和服务器建立连接,单机超链接,http开始工作</span><br><span class="line">2.建立连接后,客户机发送请求给服务器,发送格式为:</span><br><span class="line">	统一资源标识符(URL),</span><br><span class="line">	协议版本号,</span><br><span class="line">	MIME:请求修饰符,客户机信息</span><br><span class="line">3.服务器接到请求,回应响应信息,返回格式为:</span><br><span class="line">	信息协议版本号,</span><br><span class="line">	成功或错误码,</span><br><span class="line">	MIME:服务器信息,实体信息,可能内容</span><br><span class="line">4.客户机接到服务器返回信息通过浏览器显示在用户显示屏,客户机与服务器断开连接</span><br></pre></td></tr></table></figure></p>
<p>###计算10天前是几号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">import datetime</span><br><span class="line">from datetime import timedelta</span><br><span class="line">(datetime.datetime.now() - datetime.timedelta(days = 10)).strftime(&quot;%Y-%m-%d&quot;)</span><br></pre></td></tr></table></figure></p>
<p>###网络爬虫的原理 网络请求 抓取结构化数据 数据存储<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1.打开浏览器,输入URL,打开源网页</span><br><span class="line">2.选取我们想要的内容</span><br><span class="line">3.存入到硬盘中</span><br></pre></td></tr></table></figure></p>
<p>###集合<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">随机删除元素:popitem()</span><br></pre></td></tr></table></figure></p>
<p>###Apache和Tomcat比较<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">相同:</span><br><span class="line">	都是Apache组织开发的</span><br><span class="line">	都有HTTP服务功能</span><br><span class="line">	都是免费的</span><br><span class="line">不同:</span><br><span class="line">	Apache专门提供HTTP服务,以及虚拟主机,URL转发,而Tomcat是Apache组织在符合JAvaEE的标准下开发的一个JSP服务器</span><br><span class="line">	Apache是一个Web服务器环境程序,不过只支持静态网页,对于动态网页需要JSP解释器来执行动态网页</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Nginx是一个高性能的HTTP和反向代理服务器,十分轻量级的HTTP服务器</p>
</blockquote>
<p>###网页爬虫的基本工作原理<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">选取种子URl</span><br><span class="line">将代抓取URl放入代抓取URL队列</span><br><span class="line">从代抓取队列中抓取URL,解析DNS,并且得到主机ip,并将URL对应的网页下载下来,存储到已经下载的网页库中,将已经住区url存入已经抓取的URL队列</span><br></pre></td></tr></table></figure></p>
<p>###用于转化header<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ctrl+r转换头部为字典:</span><br><span class="line">(.*): (.*)  |    &amp;</span><br><span class="line">&quot;$1&quot;: &quot;$2&quot;, |    \n</span><br></pre></td></tr></table></figure></p>
<p>###积累<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1.json.dumps:将字典转化为json</span><br><span class="line">  json.loads:将json转化为字典</span><br><span class="line"></span><br><span class="line">2.没有验证码情况下模拟豆瓣登陆使用首页的地址和header头</span><br><span class="line"></span><br><span class="line">3.调用函数:</span><br><span class="line">  if __name__ == &apos;__mian__&apos;:</span><br><span class="line">        login()</span><br><span class="line"></span><br><span class="line">4.request.urlretrieve(): 下载 p1:下载地址 p2:文件保存路径</span><br><span class="line">  re.S换行符号</span><br><span class="line">  下载图片: from urllib import request</span><br><span class="line">      fname = pic.split(&apos;/&apos;)[-1]  切割后去取最后一位;</span><br><span class="line">      request.urlretrieve(pic,&apos;./images/&apos; + fname)  下载图片;</span><br><span class="line"></span><br><span class="line">5.session:存在服务器端基于cookie实现 存放形式:数据库,文件,内存</span><br><span class="line">  session多台服务器的共享: 单独建立缓存服务器,多台服务器共同使用</span><br><span class="line">  cookie:存在客户端浏览器的用户信息</span><br></pre></td></tr></table></figure></p>
<p>###cookie<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">from http import cookiejar:导入cookie管理器,存储cookie</span><br><span class="line">cookie = cookiejar.CookieJar:做个cookie对象</span><br><span class="line">cookie_hander = request.HTTPcookieProcessor(cookie) :可以管理cookie的管理器</span><br><span class="line">opener = request.build_opener(cookie_hander)</span><br></pre></td></tr></table></figure></p>
<p>##第四天  正则表达式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">1.电话 邮箱 账号 密码 IP地址 校验</span><br><span class="line"></span><br><span class="line">2.目标字符匹配</span><br><span class="line">  \d : 匹配数字(0-9),匹配一次            |  \D:非\d</span><br><span class="line">  \w : 匹配数字 字母 下划线(包括大写)     |  \W:非\w</span><br><span class="line">  \s : 匹配一个空格                      |  \S:非\s</span><br><span class="line"></span><br><span class="line">  . : 匹配任意字符</span><br><span class="line">  \ : 转义字符 (两个斜杠时,在匹配项的引号前加r转义,载原字符串也加r)</span><br><span class="line">  + : 前面的匹配项重复1-N次</span><br><span class="line">  * : 前面字符,重复0-N次</span><br><span class="line">  ? : 前面字符,重复0次或1次</span><br><span class="line"></span><br><span class="line">  &#123;n,m&#125;: 重复n-m次</span><br><span class="line">  [0-9a-zA-Z,#]: 匹配0-9a-zA-Z逗号#号,匹配任意一个</span><br><span class="line">  [0-9a-zA-Z,#]+: 匹配0-9a-zA-Z逗号#号,匹配符合全部的,到不合适的结束</span><br><span class="line">  [^]:非[]号中的</span><br><span class="line">  ():分组概念</span><br><span class="line">     group(0)</span><br><span class="line">     group(1)</span><br><span class="line">     group(2)</span><br><span class="line">  ^ : 以规则开始</span><br><span class="line">  $ : 以规则结束</span><br><span class="line"></span><br><span class="line">  注意:</span><br><span class="line">    1. . *匹配默认是贪婪模式</span><br><span class="line"></span><br><span class="line">  re.compile(): 编译规则,参数就是规则</span><br><span class="line">  match方法:从开始位置查找,一次匹配</span><br><span class="line">  search方法:检索目标字符串,返回第一次出现符合规则的字符串;为匹配到返回none;</span><br><span class="line">  findall方法:匹配全部,返回结果是列表</span><br><span class="line">  finditer方法:迭代器 只能迭代一次</span><br><span class="line">  res.group()   取出匹配到的</span><br></pre></td></tr></table></figure></p>
<p>##day5:反反爬<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">1.headers</span><br><span class="line">2.cookie</span><br><span class="line">3.USER_AGENT</span><br><span class="line">4.proxy 代理</span><br><span class="line">    1.网上获取免费代理</span><br><span class="line">        西刺:速度越快,时间越短</span><br><span class="line">	2.购买IP</span><br><span class="line">5.时间间隔 timeout timesleep</span><br><span class="line">6.并发:多线程</span><br><span class="line">    seleinum -----&gt;  phanomjs</span><br><span class="line"></span><br><span class="line">反爬虫手段:</span><br><span class="line">    urllib User-Agent;  timeout timesleep; 验证码; Pathtomjs selenium;</span><br><span class="line"></span><br><span class="line">7.print(&apos;crawing page %s&apos; % base_url) #打印获取页数路径</span><br><span class="line">  f.readlines() : 读取每一行</span><br></pre></td></tr></table></figure></p>
<p>###周一: 如何解析页面,提取数据<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">requests:安装 pip install requests</span><br><span class="line">  优点:有长链接,保存链接,节省资源 封装简单化</span><br><span class="line">  缺点:第三方封装模块,发生问题概率大于url</span><br><span class="line"></span><br><span class="line">  jsonpath:构建规则,专门提取json数据  参数1 数据参数:字典 ;参数2 匹配规则</span><br><span class="line">       jsonpath.jsonpath()</span><br><span class="line">       $:从根节点开始匹配</span><br><span class="line">       $ ..: 不管位置,选择所有适合条件的</span><br><span class="line">       *:匹配所有元素节点</span><br><span class="line">  xpath: xml: 不经常使用</span><br><span class="line">       path:转化html为节点,提取数据</span><br><span class="line">       .:当前节点</span><br><span class="line">  bs4:基于正则封装</span><br><span class="line"></span><br><span class="line">requests 模块:</span><br><span class="line">get请求:</span><br><span class="line">    params=&#123;&apos;wd&apos;,&apos;ip&apos;&#125;  传入类型为字典,放在路径后面</span><br><span class="line">    response.encoding(&apos;gbk&apos;) 转换编码格式</span><br><span class="line">post请求:</span><br><span class="line">    登陆页面 加header  加data</span><br><span class="line"></span><br><span class="line">cookie的存储:</span><br><span class="line">        建立会话</span><br><span class="line">        sess = resuests.session()</span><br><span class="line">        sess.get()   #发送get请求</span><br><span class="line">        sess.post()  #发送post请求</span><br><span class="line"></span><br><span class="line">如何加代理:</span><br><span class="line">proxy = &#123;</span><br><span class="line">    &apos;http&apos; : &apos;http://ip:端口号&apos;,</span><br><span class="line">    &apos;https&apos; : &apos;https://ip:端口号&apos;,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>##周二:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">BeautifulSoup4:</span><br><span class="line">直接获取，获取第一个</span><br><span class="line">	print(html.p.attrs) ;获取p的所有属性，</span><br><span class="line">	print（html.p[&apos;class&apos;]）;获取单个属性</span><br><span class="line">	</span><br><span class="line">	html.p.find_all(&apos;b&apos;); 获取所有p标签下的b标签</span><br><span class="line">	html.find_all(&apos;p&apos;,&apos;b&apos;);可以传入多个标记</span><br><span class="line">	</span><br><span class="line">	html.find_all(id=&apos;link1&apos;);根据id获取</span><br><span class="line">	html.find_all(&apos;p&apos;,attrs=&#123;&apos;class&apos;:&apos;title&apos;&#125;);根据属性过滤标记，attrs,需要传入字典</span><br><span class="line"></span><br><span class="line">CSS选择器：</span><br><span class="line"></span><br><span class="line">	html.select(&apos;p.title&apos;)；类过滤class</span><br><span class="line">	html.select(&apos;a#link1&apos;); id过滤 id</span><br><span class="line">	html.select(&apos;&apos;)</span><br><span class="line">	html.select(&apos;p[class=&quot;title&quot;]&apos;); 单个查询；</span><br><span class="line">	</span><br><span class="line">	html.select(&apos;p[class*=&quot;&quot;]&apos;);模糊查询；</span><br><span class="line">	html.select(&apos;p[class=&quot;&quot;]&apos;);模糊查询；</span><br><span class="line">	html.select(&apos;p[class*=&quot;&quot;]&apos;);模糊查询；</span><br><span class="line"></span><br><span class="line">content ;取内容和标签；</span><br><span class="line"></span><br><span class="line">解析数据:</span><br><span class="line">    xpath; BeautifulSoup;lxml;CSS选择器</span><br></pre></td></tr></table></figure></p>
<p>##周三: 并发量:提高采集效率<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">多进程:cpu资源分配的最小单位</span><br><span class="line">	  适合计算密集型应用 cpu密集型(对cpu消耗非常大)</span><br><span class="line">	</span><br><span class="line">线程: cpu调度的最小单位</span><br><span class="line">	  线程之间调度由操作系统来决定</span><br><span class="line">	  适合IO(输入输出 input output) 密集型应用</span><br><span class="line">	  爬虫主要就是网络IO(request:输出 response:输入)</span><br><span class="line">	</span><br><span class="line">协程:轻量级线程,单进程,单线程,写程序(代码)来调度(切换)程序</span><br><span class="line">	</span><br><span class="line">分布式:数据量非常大(PB),把程序部署到多台机器上</span><br><span class="line">	  优点:并发量大</span><br><span class="line">	  缺点:成本较高;网络带宽限制分布式爬虫</span><br><span class="line"></span><br><span class="line">#线程模块:</span><br><span class="line">import threading</span><br><span class="line"></span><br><span class="line">    线程函数传入参数</span><br><span class="line">    t1 = threading.Thread(target=foo3,args=(1,))</span><br><span class="line"></span><br><span class="line">    join();确保运行完毕,继续运行进程,阻塞进程,串行</span><br><span class="line">        1.join方法主要是阻塞住进程(无法执行join语句后的语句),专注多进程</span><br><span class="line">        2.多线程情况下,前一个结束才执行后面一个</span><br><span class="line">        3.无参数,则等待该线程结束,才开始执行下一个线程的join</span><br><span class="line">        4.</span><br><span class="line">    t.is_alive();判断线程存活状态 ,若存活返回true,若不存活返回flase</span><br><span class="line"></span><br><span class="line">    栈: 先进后出,后进先出 递归</span><br><span class="line">    队列:先进先出,后进后出,线程安全,操作后锁定</span><br><span class="line">    列表;非线程安全的</span><br><span class="line">    super(Crawl,self ) 调用父类初始化方法;</span><br><span class="line"></span><br><span class="line">#队列模块:</span><br><span class="line">import queue</span><br><span class="line">    q = queue.Queue()  队列;</span><br><span class="line">    q.put(1)  放数据;</span><br><span class="line">    q.get(1)  取数据;</span><br><span class="line">    q.get(timeout) 设置超时时间;</span><br><span class="line">    q.qsize  求出队列长度;</span><br><span class="line"></span><br><span class="line">创建队列 初始化在def __init__()</span><br></pre></td></tr></table></figure></p>
<p>###周四:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">反爬虫手段:</span><br><span class="line">    遇到ajax,请求 抓包工具,找到数据接口(最理想的)</span><br><span class="line">    自动化测试工具 配合 浏览器来完成模拟用户真实操作自动化</span><br><span class="line">把Selenium和PhantomJS结合在一起,是一个强大的网络爬虫,可以处理JavaScrip,Cookie,headers,以及我们真实用户实现的任何操作</span><br><span class="line"></span><br><span class="line">Selenium: 可以按照指定的命令自动操作,可以按照指令自行加载页面,进行截屏</span><br><span class="line">          Selenium自己不带浏览器,不支持浏览器功能,需要与第三方浏览器结合在一起才能使用</span><br><span class="line">          但我们有时候需要内嵌代码执行,</span><br><span class="line"></span><br><span class="line">PhantomJS;无界面的浏览器,它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效。</span><br><span class="line">WebDriver :WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，</span><br><span class="line">        与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫。</span><br><span class="line"></span><br><span class="line">    理念:所见即所得</span><br><span class="line">    browser---selenium(自动化测试工具) phantomsjs</span><br><span class="line">    selenium 安装;</span><br><span class="line">        pip install selenium</span><br><span class="line">    phantomsjs安装:</span><br><span class="line">        解压</span><br><span class="line">        配置环境变量:</span><br><span class="line">        系统变量Path : bin目录</span><br><span class="line"></span><br><span class="line">     按键精灵 ---自动打怪:提供很多鼠标和键盘的API接口</span><br><span class="line"></span><br><span class="line">#导入webdriver</span><br><span class="line">#from selenium import webdriver</span><br><span class="line">    browser.webdriver.PhantomJS()  #调用环境变量指定的phantomJS浏览器创建浏览对象</span><br><span class="line">    browser.get(&apos;网页地址&apos;)</span><br><span class="line">    time.sleep(1)   请求1秒</span><br><span class="line">    print(browser.page_source)  输出爬到的页面</span><br><span class="line">    # key是要传入的参数 by_name;可以根据name属性查找</span><br><span class="line">    browser.find_element_by_id(&apos;kw&apos;).send_keys(&apos;双十一&apos;)   根据id找到相应标签,传入要搜素的内容</span><br><span class="line">    browser.save_screenshot(&apos;baidu.png&apos;) 查看浏览器状态,截图显示;</span><br><span class="line">    browser.find_element_by_id(&apos;su&apos;).click()  找到百度知道,然后点击</span><br><span class="line">    browser.execute_script()  执行js代码</span><br><span class="line">    time.sleep(1)</span><br><span class="line">查找类:</span><br><span class="line">    BeautifulSoup 取数据时候: 属性id # ; 属性class .</span><br><span class="line">    span[class*=&quot;tag ellipsis&quot;][0].text 取span下的复合class的值;</span><br><span class="line"></span><br><span class="line">mysql数据库:</span><br><span class="line">    1.连接数据库         conn = pymysql.connect(ip,账号,密码,库,编码)</span><br><span class="line">    2.创建数据库操作对象</span><br><span class="line">    3.构建sql语句</span><br><span class="line">    4.执行sql语句  sql = &apos;select version()&apos;</span><br><span class="line">    5.获取执行结果</span><br><span class="line">        增删改:影响行数</span><br><span class="line">        查询:获取结果集</span><br><span class="line"></span><br><span class="line">    增删改操作需要提交;</span><br><span class="line">    conn.commit()  提交事物;</span><br><span class="line">    mydb.exe(sql) 执行语句</span><br></pre></td></tr></table></figure></p>
<p>##day 5:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">    php集成开发环境里面有mysql:lnmp lamp negix</span><br><span class="line">	selenium包的webdriver 自动化获取;</span><br><span class="line"></span><br><span class="line">百度图片瀑布流:</span><br><span class="line">    document.body.scrollHeight  页面高度;</span><br><span class="line">    scrollTo(0,document.body.scrollHeight)  页面滚动到底部;</span><br><span class="line">    scrollTo(0,0)  页面滚动到顶部;</span><br><span class="line"></span><br><span class="line">    browser.execute_script()  执行js代码</span><br><span class="line"></span><br><span class="line">XPATH:</span><br><span class="line">from lxml import etree:   导入包,使用的是直接找div的方法</span><br><span class="line">    ./image/@src  获取图片的src属性; xpath</span><br><span class="line">    在scrapy中使用xpath要在末尾加 .extract()</span><br><span class="line"></span><br><span class="line">CSS选择器:</span><br><span class="line">    name = teacher.css(&apos;h1::text&apos;).extract()[0] 获取文本</span><br><span class="line">    image = teacher.css(&apos;img::attr(src)&apos;).extract()[0]</span><br><span class="line"></span><br><span class="line">报错原因:</span><br><span class="line">    10061:网络问题</span><br><span class="line">    403 For:服务器拒绝访问</span><br><span class="line"></span><br><span class="line">    scrapy Engine(引擎):负责Spider,ItemPipeline,Downloader,Scheduler中间的通讯,信号,数据传递等</span><br><span class="line">    Scheduler(调度器):负责搜素引擎发送过来的Request请求,并按照一定的方式进行整理队列,入队(Queue),当引擎需要时,交换给引擎</span><br><span class="line">    Downloader(下载器):负责下载scrapy Engine(引擎)发送的Request请求,并且将获取到的Response,交换给引擎,由引擎交给Spider来处理</span><br><span class="line">    Spider(爬虫):它负责处理所以Response,从中分析提取数据,获取item字段需要的数据,并将需要跟进的URL提交给引擎,再进入Scheduler调度器</span><br><span class="line">    Item Pipeline(管道):它负责处理Spider中获取item,并且进行后期处理(详细分析,过滤,存储)</span><br><span class="line">    Downloader Middlewares(下载中间件):自定义扩展功能的组件</span><br><span class="line">    Spider Middlewares(Spider中间件):中间通信</span><br><span class="line">    Item: 定义爬取的字段;</span><br><span class="line"></span><br><span class="line">爬虫四步骤:&apos;</span><br><span class="line">    创建一个新的爬虫项目(Scrapy startproject xxx)</span><br><span class="line">    明确你要爬取的目标 (编写items.py)</span><br><span class="line">    制作爬虫开始爬取网页: spiders/xxspider.py</span><br><span class="line">    设计管道存储爬取内容: pipelines.py</span><br><span class="line"></span><br><span class="line"> 爬取时加参数-o: scrapy crawl xdl -o teacher.json</span><br><span class="line"></span><br><span class="line">关于爬虫的建议:</span><br><span class="line">    尽量减少请求次数,能抓列表页不抓详情页,减轻服务器压力;</span><br><span class="line">    反爬手段可以适用web,手机App和H5</span><br><span class="line">    尽量少用ip去做防守</span><br><span class="line">    要求高性能,可以考虑多线程(Scrapy框架支持),分布式</span><br><span class="line"></span><br><span class="line">加锁, 释放锁;</span><br><span class="line"></span><br><span class="line">python语言的优点;</span><br><span class="line">    优雅,明确,简单</span><br><span class="line">    开发效率高,第三方库强大,降低开发周期,避免重复造车</span><br><span class="line">    高级语言,无需考虑底层实现</span><br><span class="line">    可移植性强,可以在不同平台运行</span><br><span class="line">    可扩展性,可以运用C 或者其他编写,在python中运行</span><br><span class="line">    可嵌入性,可嵌入C/C++,</span><br><span class="line">缺点:</span><br><span class="line">    速度慢,但是对于调试工具而言,用户无法感知</span><br><span class="line">    代码不能加密,因为是解释性语言,代码明文存在</span><br><span class="line"></span><br><span class="line">python中的程序分析包,可以检测程序的运行速度: cProfile</span><br></pre></td></tr></table></figure></p>
<blockquote>
<p>Request:url地址，方法，headers,cookie</p>
</blockquote>
<p>##day3:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">反爬虫手段:</span><br><span class="line">    urllib User-Agent 随机的选择浏览器身份 ;</span><br><span class="line">    proxy 随机挑选代理发起请求代理 ;</span><br><span class="line">    timeout timesleep; 验证码; Pathtomjs selenium;</span><br><span class="line"></span><br><span class="line">如何导入setting配置:</span><br><span class="line">    如何导入settings 配置</span><br><span class="line">    1.</span><br><span class="line">    from py03_spider_day13 import settings</span><br><span class="line">    settings.USER_AGENTS</span><br><span class="line">    2.</span><br><span class="line">    from scrapy.conf import settings</span><br><span class="line">    settings[&apos;USER_AGENTS&apos;]</span><br><span class="line">    3.</span><br><span class="line">    @classmethod</span><br><span class="line">    def from_crawler(cls,crawler):</span><br><span class="line">        return cls(crawler)</span><br><span class="line"></span><br><span class="line">管理user-agent的包:fake-useragent</span><br><span class="line">    ua.random  随机 user-agent;</span><br><span class="line"></span><br><span class="line">一个中间件就是一个类,静态方法先加载</span><br><span class="line">__init__()方法是一种特殊的方法,被称为类的构造函数或初始化方法,当创建这个类的实例时调用该方法</span><br><span class="line"></span><br><span class="line">redis:缓存型数据库,数据存在内存中</span><br><span class="line">    1.指纹识别机制</span><br><span class="line">    2.统一管理请求队列</span><br><span class="line">    3.爬取数据统一管理</span><br><span class="line"></span><br><span class="line">Scrapy框架:</span><br><span class="line"></span><br><span class="line">     进入：(win)Scripts\activate</span><br><span class="line">	       (linux)source activate</span><br><span class="line">     scrapy startproject scrpy_test 创建项目</span><br><span class="line">     scrapy genspider baidu www.baidu.com   需要进入项目目录</span><br><span class="line">     scrapy genspider -t crawl baidu www.baidu.com</span><br><span class="line">     scrapy crawl baidu      启动项目</span><br><span class="line"></span><br><span class="line">     split 以指定字符切割字符串 转换成列表</span><br><span class="line">     strip() 方法用于移除字符串头尾指定的字符（默认为空格）转换后成列表</span><br><span class="line">     replace() 方法把字符串中的 old（旧字符串） 替换成 new(新字符串)，如果指定第三个参数max，则替换不超过 max</span><br><span class="line">	 strip() 去掉空格;</span><br><span class="line">     replace(&apos;万&apos;,&apos;&apos;)  去掉万;</span><br><span class="line"></span><br><span class="line">打开redis方法:</span><br><span class="line">在setting文件中配置:</span><br><span class="line">  </span><br><span class="line">    Linux:</span><br><span class="line">        cd redis</span><br><span class="line">        cd src</span><br><span class="line">        ./redis-server /etc/redis.conf</span><br><span class="line"></span><br><span class="line">    window:</span><br><span class="line">		redis-server.exe redis.windows.conf</span><br><span class="line">        redis-cli -h 192.168.103.110</span><br><span class="line">        lpush liepinspider:urls https://www.liepin.com</span><br><span class="line">        keys *</span><br></pre></td></tr></table></figure></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2017/11/26/Scrapy/" data-id="cjaghm3m50000h4v1oyupk8r5" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2017/11/26/linux总结/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title"></div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list"><li class="tag-list-item"><a class="tag-list-link" href="/tags/django/">django</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/django/" style="font-size: 10px;">django</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/11/">November 2017</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/">May 2017</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2017/11/26/Scrapy/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/11/26/linux总结/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/11/26/MySql/">(no title)</a>
          </li>
        
          <li>
            <a href="/2017/11/26/hello-world/">Hello World</a>
          </li>
        
          <li>
            <a href="/2017/05/22/django留言板/">django留言板</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2017 John Doe<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>

  </div>
</body>
</html>